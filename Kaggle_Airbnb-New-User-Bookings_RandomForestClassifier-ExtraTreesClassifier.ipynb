{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dataset from Kaggle Project - Airbnb\n",
    "#### https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name: Jianlei(John) Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from datetime import datetime, date\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "% matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the datasets\n",
    "train_users = pd.read_csv('./data_HW5/train_users_2.csv.zip')\n",
    "age_gender_bkts = pd.read_csv('./data_HW5/age_gender_bkts.csv.zip')\n",
    "countries = pd.read_csv('./data_HW5/countries.csv.zip')\n",
    "sessions = pd.read_csv('./data_HW5/sessions.csv.zip')\n",
    "test_users = pd.read_csv('./data_HW5/test_users.csv.zip')\n",
    "sample_submission_NDF = pd.read_csv('./data_HW5/sample_submission_NDF.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_account_created</th>\n",
       "      <th>timestamp_first_active</th>\n",
       "      <th>date_first_booking</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>signup_method</th>\n",
       "      <th>signup_flow</th>\n",
       "      <th>language</th>\n",
       "      <th>affiliate_channel</th>\n",
       "      <th>affiliate_provider</th>\n",
       "      <th>first_affiliate_tracked</th>\n",
       "      <th>signup_app</th>\n",
       "      <th>first_device_type</th>\n",
       "      <th>first_browser</th>\n",
       "      <th>country_destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gxn3p5htnn</td>\n",
       "      <td>2010-06-28</td>\n",
       "      <td>20090319043255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820tgsjxq7</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>20090523174809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MALE</td>\n",
       "      <td>38.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>seo</td>\n",
       "      <td>google</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id date_account_created  timestamp_first_active date_first_booking  \\\n",
       "0  gxn3p5htnn           2010-06-28          20090319043255                NaN   \n",
       "1  820tgsjxq7           2011-05-25          20090523174809                NaN   \n",
       "\n",
       "      gender   age signup_method  signup_flow language affiliate_channel  \\\n",
       "0  -unknown-   NaN      facebook            0       en            direct   \n",
       "1       MALE  38.0      facebook            0       en               seo   \n",
       "\n",
       "  affiliate_provider first_affiliate_tracked signup_app first_device_type  \\\n",
       "0             direct               untracked        Web       Mac Desktop   \n",
       "1             google               untracked        Web       Mac Desktop   \n",
       "\n",
       "  first_browser country_destination  \n",
       "0        Chrome                 NDF  \n",
       "1        Chrome                 NDF  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_users.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train_users['country_destination'].values\n",
    "train_data = train_users.drop(['country_destination'], axis = 1)\n",
    "\n",
    "data = pd.concat((train_data, test_users), axis = 0, ignore_index = True)\n",
    "data_all = data.fillna(-1)\n",
    "data_all = data_all.replace('-unknown-', -1)\n",
    "data_all = data_all.drop(['date_first_booking'], axis = 1)\n",
    "\n",
    "# check number of null valus in each record\n",
    "data_all['n_null'] = np.array([sum(r == -1) for r in data_all.values]) \n",
    "\n",
    "# 1) data_account_created \n",
    "dac = np.vstack(data_all['date_account_created'].astype(str)\n",
    "                .apply(lambda x: list(map(int, x.split('-')))).values)\n",
    "data_all['dac_year'] = dac[:,0]\n",
    "data_all['dac_month'] = dac[:,1]\n",
    "data_all['dac_day'] = dac[:,2]\n",
    "\n",
    "dac_dates = [datetime(x[0],x[1],x[2]) for x in dac]\n",
    "data_all['dac_wn'] = np.array([d.isocalendar()[1] for d in dac_dates])\n",
    "data_all['dac_w'] = np.array([d.weekday() for d in dac_dates])\n",
    "t = pd.get_dummies(data_all.dac_w, prefix='dac_w')\n",
    "data_all = pd.concat((data_all, t), axis=1)\n",
    "data_all = data_all.drop(['date_account_created', 'dac_w'], axis=1)\n",
    "\n",
    "# 2) timestamp_first_active\n",
    "tfa = np.vstack(data_all['timestamp_first_active'].astype(str)\n",
    "                .apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values)\n",
    "data_all['tfa_year'] = tfa[:,0]\n",
    "data_all['tfa_month'] = tfa[:,1]\n",
    "data_all['tfa_day'] = tfa[:,2]\n",
    "data_all['tfa_hour'] = tfa[:,3]\n",
    "\n",
    "tfa_dates = [datetime(x[0],x[1],x[2],x[3],x[4],x[5]) for x in tfa]\n",
    "data_all['tfa_wn'] = np.array([d.isocalendar()[1] for d in tfa_dates])\n",
    "data_all['tfa_w'] = np.array([d.weekday() for d in tfa_dates])\n",
    "t = pd.get_dummies(data_all.tfa_w, prefix='tfa_w')\n",
    "data_all = pd.concat((data_all, t), axis=1)\n",
    "data_all = data_all.drop(['timestamp_first_active', 'tfa_w'], axis=1)\n",
    "\n",
    "# 3) timespans between 'dac' and 'tfa'\n",
    "data_all['dac_tfa_secs'] = np.array([np.log(1+abs((dac_dates[i]-tfa_dates[i]).total_seconds())) \n",
    "                                  for i in range(len(dac_dates))])\n",
    "data_all['sig_dac_tfa'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).total_seconds()) \n",
    "                                 for i in range(len(dac_dates))])\n",
    "\n",
    "# 4) seaons for 'dac' and 'tfa'\n",
    "Y = 2000 # dummy year\n",
    "seasons = [(0, (date(Y,  1,  1),  date(Y,  3, 20))),  #'winter'\n",
    "           (1, (date(Y,  3, 21),  date(Y,  6, 20))),  #'spring'\n",
    "           (2, (date(Y,  6, 21),  date(Y,  9, 22))),  #'summer'\n",
    "           (3, (date(Y,  9, 23),  date(Y, 12, 20))),  #'autumn'\n",
    "           (0, (date(Y, 12, 21),  date(Y, 12, 31)))]  #'winter'\n",
    "def get_season(dt):\n",
    "    dt = dt.date()\n",
    "    dt = dt.replace(year=Y)\n",
    "    return next(season for season, (start, end) in seasons\n",
    "                if start <= dt <= end)\n",
    "data_all['season_dac'] = np.array([get_season(dt) for dt in dac_dates])\n",
    "data_all['season_tfa'] = np.array([get_season(dt) for dt in tfa_dates])\n",
    "\n",
    "# 5) group 'age'\n",
    "\n",
    "# OK for'ages' in 14 < age < 99, otherwise group 'ages'\n",
    "av = data_all['age'].values\n",
    "av = np.where(np.logical_and(av<2000, av>1900), 2014-av, av) # birthdays \n",
    "av = np.where(np.logical_and(av<14, av>0), 4, av)\n",
    "av = np.where(np.logical_and(av<2016, av>2010), 9, av) # 9 for current years\n",
    "av = np.where(av > 99, 110, av)  # 110 for age values above 99\n",
    "data_all['age'] = av\n",
    "\n",
    "# create ranges for 'age' and One-hot encoding \n",
    "interv =  [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 100]\n",
    "def get_interv_value(age):\n",
    "    iv = 20\n",
    "    for i in range(len(interv)):\n",
    "        if age < interv[i]:\n",
    "            iv = i \n",
    "            break\n",
    "    return iv\n",
    "data_all['age_interv'] = data_all.age.apply(lambda x: get_interv_value(x))\n",
    "t = pd.get_dummies(data_all.age_interv, prefix='age_interv')\n",
    "data_all = pd.concat((data_all, t), axis=1)\n",
    "\n",
    "data_all = data_all.drop(['age_interv'], axis=1)\n",
    "\n",
    "# 6) One-hot-encoding other categorical features\n",
    "ohe_features = ['gender', 'signup_method', 'signup_flow', 'language', \n",
    "             'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', \n",
    "             'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for f in ohe_features:\n",
    "    data_all_dummies = pd.get_dummies(data_all[f], prefix=f)\n",
    "    data_all = data_all.drop([f], axis=1)\n",
    "    data_all = pd.concat((data_all, data_all_dummies), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 from 135483\n",
      "10000 from 135483\n",
      "20000 from 135483\n",
      "30000 from 135483\n",
      "40000 from 135483\n",
      "50000 from 135483\n",
      "60000 from 135483\n",
      "70000 from 135483\n",
      "80000 from 135483\n",
      "90000 from 135483\n",
      "100000 from 135483\n",
      "110000 from 135483\n",
      "120000 from 135483\n",
      "130000 from 135483\n"
     ]
    }
   ],
   "source": [
    "# 7) session\n",
    "sessions['id'] = sessions['user_id']\n",
    "sessions = sessions.drop(['user_id'], axis = 1)\n",
    "\n",
    "# fill missing values with 'NAN'\n",
    "sessions['action'] = sessions.action.fillna('NAN')\n",
    "sessions['action_type'] = sessions.action_type.fillna('NAN')\n",
    "sessions['action_detail'] = sessions.action_detail.fillna('NAN')\n",
    "sessions['device_type'] = sessions.device_type.fillna('NAN')\n",
    "\n",
    "# assign \"OTHER\" to the low frequency \"action\" iterms\n",
    "actFreq = 100 # JIANLEI_DEFINED: frequency threhold \n",
    "actDict = dict(zip(*np.unique(sessions.action, return_counts=True)))\n",
    "sessions['action'] = sessions.action.apply(lambda x: 'OTHER' if actDict[x] < actFreq else x)\n",
    "\n",
    "f_act = sessions.action.value_counts().argsort()\n",
    "f_act_detail = sessions.action_detail.value_counts().argsort()\n",
    "f_act_type = sessions.action_type.value_counts().argsort()\n",
    "f_dev_type = sessions.device_type.value_counts().argsort()\n",
    "\n",
    "# group session by 'id'\n",
    "dgr_sess = sessions.groupby(['id'])\n",
    "# Loop on dgr_sess to create all the features\n",
    "samples = []\n",
    "cont = 0\n",
    "ln = len(dgr_sess)\n",
    "for g in dgr_sess:\n",
    "    if cont%10000 == 0:\n",
    "        print(\"%s from %s\" %(cont, ln))\n",
    "    gr = g[1]\n",
    "    l = []\n",
    "    \n",
    "    # add the 'id'\n",
    "    l.append(g[0])\n",
    "    \n",
    "    # add the number of values\n",
    "    l.append(len(gr))\n",
    "    \n",
    "    sev = gr.secs_elapsed.fillna(0).values \n",
    "    \n",
    "    # add 'action' features\n",
    "    c_act = [0] * len(f_act)\n",
    "    for i,v in enumerate(gr.action.values):\n",
    "        c_act[f_act[v]] += 1\n",
    "    _, c_act_uqc = np.unique(gr.action.values, return_counts=True)\n",
    "    c_act += [len(c_act_uqc), np.mean(c_act_uqc), np.std(c_act_uqc)]\n",
    "    l = l + c_act\n",
    "    \n",
    "    # add 'action_detail' features\n",
    "    c_act_detail = [0] * len(f_act_detail)\n",
    "    for i,v in enumerate(gr.action_detail.values):\n",
    "        c_act_detail[f_act_detail[v]] += 1 \n",
    "    _, c_act_det_uqc = np.unique(gr.action_detail.values, return_counts=True)\n",
    "    c_act_detail += [len(c_act_det_uqc), np.mean(c_act_det_uqc), np.std(c_act_det_uqc)]\n",
    "    l = l + c_act_detail\n",
    "    \n",
    "    # add 'action_type' features\n",
    "    l_act_type = [0] * len(f_act_type)\n",
    "    c_act_type = [0] * len(f_act_type)\n",
    "    for i,v in enumerate(gr.action_type.values):\n",
    "        l_act_type[f_act_type[v]] += sev[i]   \n",
    "        c_act_type[f_act_type[v]] += 1  \n",
    "    l_act_type = np.log(1 + np.array(l_act_type)).tolist()\n",
    "    _, c_act_type_uqc = np.unique(gr.action_type.values, return_counts=True)\n",
    "    c_act_type += [len(c_act_type_uqc), np.mean(c_act_type_uqc), np.std(c_act_type_uqc)]\n",
    "    l = l + c_act_type + l_act_type    \n",
    "    \n",
    "    # add 'device_type' features\n",
    "    c_dev_type  = [0] * len(f_dev_type)\n",
    "    for i,v in enumerate(gr.device_type .values):\n",
    "        c_dev_type[f_dev_type[v]] += 1 \n",
    "    _, c_dev_type_uqc = np.unique(gr.device_type.values, return_counts=True)\n",
    "    c_dev_type += [len(c_dev_type_uqc), np.mean(c_dev_type_uqc), np.std(c_dev_type_uqc)]        \n",
    "    l = l + c_dev_type    \n",
    "    \n",
    "    # add 'secs_elapsed' features        \n",
    "    l_secs = [0] * 5 # five simple statistics\n",
    "    l_log = [0] * 15 # group the values into 15 intervals\n",
    "    if len(sev) > 0:\n",
    "        l_secs[0] = np.log(1 + np.sum(sev))\n",
    "        l_secs[1] = np.log(1 + np.mean(sev)) \n",
    "        l_secs[2] = np.log(1 + np.std(sev))\n",
    "        l_secs[3] = np.log(1 + np.median(sev))\n",
    "        l_secs[4] = l_secs[0] / float(l[1])\n",
    "        \n",
    "        log_sev = np.log(1 + sev).astype(int)\n",
    "        l_log = np.bincount(log_sev, minlength=15).tolist()                      \n",
    "    l = l + l_secs + l_log\n",
    "    \n",
    "    # add one sample's all features\n",
    "    samples.append(l)\n",
    "    cont += 1\n",
    "\n",
    "# Create a dataframe with the computed features    \n",
    "col_names = []    \n",
    "for i in range(len(samples[0])-1):\n",
    "    col_names.append('c_' + str(i)) \n",
    "    \n",
    "samples = np.array(samples)\n",
    "samp_ar = samples[:, 1:].astype(np.float16)\n",
    "samp_id = samples[:, 0]  \n",
    "      \n",
    "df_agg_sess = pd.DataFrame(samp_ar, columns=col_names)\n",
    "df_agg_sess['id'] = samp_id\n",
    "df_agg_sess.index = df_agg_sess.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8) comine 'train, test' with 'sessions'\n",
    "data_all = pd.merge(data_all, df_agg_sess, how='left')\n",
    "data_all = data_all.drop(['id'], axis=1)\n",
    "data_all = data_all.fillna(-2)  # Missing features for 'samples' without sesssion data.\n",
    "\n",
    "data_all['all_null'] = np.array([sum(r<0) for r in data_all.values]) # add all null types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9) split the \"data_all\" back to \"train_data\" and \"test_data\"\n",
    "X_train = data_all.values[:train_users.shape[0]]\n",
    "X_test = data_all.values[train_users.shape[0]:]\n",
    "LE = LabelEncoder()\n",
    "y = LE.fit_transform(Y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split 'X_train' data into sub_training_data and validation_data\n",
    "\n",
    "np.random.seed(2016)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     99613\n",
       "10    49952\n",
       "11     8083\n",
       "4      3995\n",
       "6      2241\n",
       "5      1842\n",
       "3      1804\n",
       "1      1144\n",
       "2       859\n",
       "8       625\n",
       "0       423\n",
       "9       179\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(y_train, columns=['Y'])\n",
    "a.Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       116\n",
      "          1       0.00      0.00      0.00       284\n",
      "          2       0.00      0.00      0.00       202\n",
      "          3       0.02      0.00      0.00       445\n",
      "          4       0.04      0.01      0.01      1028\n",
      "          5       0.02      0.00      0.00       482\n",
      "          6       0.05      0.01      0.01       594\n",
      "          7       0.68      0.83      0.75     24930\n",
      "          8       0.00      0.00      0.00       137\n",
      "          9       0.00      0.00      0.00        38\n",
      "         10       0.47      0.44      0.45     12424\n",
      "         11       0.06      0.01      0.01      2011\n",
      "\n",
      "avg / total       0.54      0.61      0.57     42691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 1) build the baseline prediction model - \"Random Forest Tree Model\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "RF = RandomForestClassifier(random_state = 2016)\n",
    "RF = RF.fit(x_train, y_train)\n",
    "y_pred = RF.predict(x_val)\n",
    "\n",
    "print metrics.classification_report(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] n_estimators=11, criterion=gini .................................\n",
      "[CV] n_estimators=11, criterion=gini .................................\n",
      "[CV] n_estimators=11, criterion=gini .................................\n",
      "[CV] n_estimators=21, criterion=gini .................................\n",
      "[CV] n_estimators=21, criterion=gini .................................\n",
      "[CV] n_estimators=21, criterion=gini .................................\n",
      "[CV] n_estimators=41, criterion=gini .................................\n",
      "[CV] n_estimators=41, criterion=gini .................................\n",
      "[CV] .. n_estimators=11, criterion=gini, score=0.616408, total=  27.4s\n",
      "[CV] n_estimators=41, criterion=gini .................................\n",
      "[CV] .. n_estimators=11, criterion=gini, score=0.614259, total=  26.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   34.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=101, criterion=gini ................................\n",
      "[CV] .. n_estimators=11, criterion=gini, score=0.610602, total=  30.8s\n",
      "[CV] n_estimators=101, criterion=gini ................................\n",
      "[CV] .. n_estimators=21, criterion=gini, score=0.626034, total=  51.1s\n",
      "[CV] n_estimators=101, criterion=gini ................................\n",
      "[CV] .. n_estimators=21, criterion=gini, score=0.624607, total=  50.8s\n",
      "[CV] .. n_estimators=21, criterion=gini, score=0.623269, total=  50.4s\n",
      "[CV] n_estimators=11, criterion=entropy ..............................\n",
      "[CV] n_estimators=11, criterion=entropy ..............................\n",
      "[CV]  n_estimators=11, criterion=entropy, score=0.613544, total=  24.0s\n",
      "[CV] n_estimators=11, criterion=entropy ..............................\n",
      "[CV]  n_estimators=11, criterion=entropy, score=0.612888, total=  24.6s\n",
      "[CV] n_estimators=21, criterion=entropy ..............................\n",
      "[CV] .. n_estimators=41, criterion=gini, score=0.630738, total= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=21, criterion=entropy ..............................\n",
      "[CV] .. n_estimators=41, criterion=gini, score=0.631023, total= 1.5min\n",
      "[CV] n_estimators=21, criterion=entropy ..............................\n",
      "[CV] .. n_estimators=41, criterion=gini, score=0.630473, total= 1.4min\n",
      "[CV] n_estimators=41, criterion=entropy ..............................\n",
      "[CV]  n_estimators=11, criterion=entropy, score=0.611533, total=  25.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.0min remaining:  2.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=41, criterion=entropy ..............................\n",
      "[CV]  n_estimators=21, criterion=entropy, score=0.625015, total=  43.2s\n",
      "[CV] n_estimators=41, criterion=entropy ..............................\n",
      "[CV]  n_estimators=21, criterion=entropy, score=0.623816, total=  43.4s\n",
      "[CV] n_estimators=101, criterion=entropy .............................\n",
      "[CV]  n_estimators=21, criterion=entropy, score=0.624288, total=  44.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.6min remaining:  1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=101, criterion=entropy .............................\n",
      "[CV]  n_estimators=41, criterion=entropy, score=0.631199, total= 1.3min\n",
      "[CV] n_estimators=101, criterion=entropy .............................\n",
      "[CV]  n_estimators=41, criterion=entropy, score=0.631652, total= 1.4min\n",
      "[CV]  n_estimators=41, criterion=entropy, score=0.630315, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.8min remaining:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . n_estimators=101, criterion=gini, score=0.634695, total= 3.2min\n",
      "[CV] . n_estimators=101, criterion=gini, score=0.635394, total= 3.2min\n",
      "[CV] . n_estimators=101, criterion=gini, score=0.634145, total= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  4.2min remaining:   36.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=101, criterion=entropy, score=0.634080, total= 2.2min\n",
      "[CV]  n_estimators=101, criterion=entropy, score=0.634937, total= 2.2min\n",
      "[CV]  n_estimators=101, criterion=entropy, score=0.633477, total= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.635\n",
      "('Best parameters set:', {'n_estimators': 101, 'criterion': 'gini'})\n",
      "('Scores:', [mean: 0.61376, std: 0.00240, params: {'n_estimators': 11, 'criterion': 'gini'}, mean: 0.62464, std: 0.00113, params: {'n_estimators': 21, 'criterion': 'gini'}, mean: 0.63074, std: 0.00022, params: {'n_estimators': 41, 'criterion': 'gini'}, mean: 0.63474, std: 0.00051, params: {'n_estimators': 101, 'criterion': 'gini'}, mean: 0.61266, std: 0.00084, params: {'n_estimators': 11, 'criterion': 'entropy'}, mean: 0.62437, std: 0.00049, params: {'n_estimators': 21, 'criterion': 'entropy'}, mean: 0.63106, std: 0.00056, params: {'n_estimators': 41, 'criterion': 'entropy'}, mean: 0.63416, std: 0.00060, params: {'n_estimators': 101, 'criterion': 'entropy'}])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py27/lib/python2.7/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 2) optimize the hyperparameters of the baseline - \"Random Forest Tree Model\"\n",
    "\n",
    "# define a function call to \"GridSearchCV\"; to search for the best model parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "np.random.seed(2016)\n",
    "\n",
    "model = GridSearchCV(estimator  = RandomForestClassifier(),\n",
    "                     param_grid = {'n_estimators':[11, 21, 41, 101], 'criterion':['gini', 'entropy']},\n",
    "                     verbose = 10, n_jobs = -1, iid  = True, refit = False)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\", model.best_params_)\n",
    "print(\"Scores:\", model.grid_scores_)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] n_estimators=21, criterion=gini .................................\n",
      "[CV] n_estimators=21, criterion=gini .................................\n",
      "[CV] n_estimators=21, criterion=gini .................................\n",
      "[CV] n_estimators=41, criterion=gini .................................\n",
      "[CV] n_estimators=41, criterion=gini .................................\n",
      "[CV] n_estimators=41, criterion=gini .................................\n",
      "[CV] n_estimators=101, criterion=gini ................................\n",
      "[CV] n_estimators=101, criterion=gini ................................\n",
      "[CV] .. n_estimators=21, criterion=gini, score=0.615968, total=  52.1s\n",
      "[CV] n_estimators=101, criterion=gini ................................\n",
      "[CV] .. n_estimators=21, criterion=gini, score=0.615208, total=  55.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=21, criterion=entropy ..............................\n",
      "[CV] .. n_estimators=21, criterion=gini, score=0.615047, total=  56.2s\n",
      "[CV] n_estimators=21, criterion=entropy ..............................\n",
      "[CV]  n_estimators=21, criterion=entropy, score=0.615881, total=  57.2s\n",
      "[CV] n_estimators=21, criterion=entropy ..............................\n",
      "[CV] .. n_estimators=41, criterion=gini, score=0.621045, total= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  18 | elapsed:  2.1min remaining:  5.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=21, criterion=entropy, score=0.614610, total=  57.2s\n",
      "[CV] n_estimators=41, criterion=entropy ..............................\n",
      "[CV] n_estimators=41, criterion=entropy ..............................\n",
      "[CV] .. n_estimators=41, criterion=gini, score=0.619509, total= 1.9min\n",
      "[CV] .. n_estimators=41, criterion=gini, score=0.621269, total= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:  2.2min remaining:  3.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=41, criterion=entropy ..............................\n",
      "[CV] n_estimators=101, criterion=entropy .............................\n",
      "[CV]  n_estimators=21, criterion=entropy, score=0.611779, total=  57.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of  18 | elapsed:  3.1min remaining:  3.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=101, criterion=entropy .............................\n",
      "[CV]  n_estimators=41, criterion=entropy, score=0.619499, total= 1.8min\n",
      "[CV] n_estimators=101, criterion=entropy .............................\n",
      "[CV]  n_estimators=41, criterion=entropy, score=0.620109, total= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  11 out of  18 | elapsed:  4.1min remaining:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=41, criterion=entropy, score=0.618508, total= 1.8min\n",
      "[CV] . n_estimators=101, criterion=gini, score=0.625173, total= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:  5.0min remaining:  1.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . n_estimators=101, criterion=gini, score=0.624308, total= 4.6min\n",
      "[CV] . n_estimators=101, criterion=gini, score=0.624693, total= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:  5.5min remaining:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=101, criterion=entropy, score=0.623188, total= 3.4min\n",
      "[CV]  n_estimators=101, criterion=entropy, score=0.623641, total= 2.9min\n",
      "[CV]  n_estimators=101, criterion=entropy, score=0.621987, total= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.625\n",
      "('Best parameters set:', {'n_estimators': 101, 'criterion': 'gini'})\n",
      "('Scores:', [mean: 0.61541, std: 0.00040, params: {'n_estimators': 21, 'criterion': 'gini'}, mean: 0.62061, std: 0.00078, params: {'n_estimators': 41, 'criterion': 'gini'}, mean: 0.62472, std: 0.00035, params: {'n_estimators': 101, 'criterion': 'gini'}, mean: 0.61409, std: 0.00171, params: {'n_estimators': 21, 'criterion': 'entropy'}, mean: 0.61937, std: 0.00066, params: {'n_estimators': 41, 'criterion': 'entropy'}, mean: 0.62294, std: 0.00070, params: {'n_estimators': 101, 'criterion': 'entropy'}])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py27/lib/python2.7/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 3) optimize the hyperparameters of the another model - \"Extra Tree Model\"\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "np.random.seed(2016)\n",
    "model = GridSearchCV(estimator  = ExtraTreesClassifier(),\n",
    "                     param_grid = {'n_estimators':[21, 41, 101], 'criterion':['gini', 'entropy']},\n",
    "                     verbose = 10, n_jobs = -1, iid  = True, refit = False)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\", model.best_params_)\n",
    "print(\"Scores:\", model.grid_scores_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=101, criterion='gini')\n",
    "ET = ExtraTreesClassifier(n_estimators=101, criterion='gini')\n",
    "\n",
    "finalModel = VotingClassifier(estimators=[('Random Forest',RF), ('Extra Tree',ET)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29 (+/- 0.15) [Random Forest]\n",
      "Accuracy: 0.37 (+/- 0.12) [Extra Eree]\n",
      "Accuracy: 0.30 (+/- 0.14) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([RF, ET, finalModel], ['Random Forest', 'Extra Eree', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X_train, y, cv=5)\n",
    "    print \"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('Random Forest', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "   ...imators=101, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False))],\n",
       "         n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalModel.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# submmit the results\n",
    "\n",
    "t = pd.DataFrame(finalModel.predict(X_test), columns=['country'])\n",
    "predict_data = pd.concat([sample_submission_NDF['id'], t], axis =1)\n",
    "predict_data.to_csv('./data_HW5/Submmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
